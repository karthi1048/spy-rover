import cv2
import numpy as np
import sounddevice as sd
import scipy.io.wavfile as wav
import threading
import time
import RPi.GPIO as GPIO  # Raspberry Pi GPIO

# GPIO setup
GPIO.setmode(GPIO.BCM)
motor_pins = [23, 24, 25, 22]
sensor_pin = 14

for pin in motor_pins:
    GPIO.setup(pin, GPIO.OUT)
    GPIO.output(pin, GPIO.LOW)

GPIO.setup(sensor_pin, GPIO.IN)

# Motor control
motor_running = True

def motor_forward():
    while motor_running:
        if GPIO.input(sensor_pin) == 0:
            print("[SENSOR] Object detected. Stopping motor.")
            stop_motor()
            #break
        else:
            print("moving forward")
            # Move forward
            GPIO.output(23, GPIO.HIGH)
            GPIO.output(24, GPIO.LOW)
            GPIO.output(25, GPIO.HIGH)
            GPIO.output(22, GPIO.LOW)
            time.sleep(0.1)
            GPIO.output(23, GPIO.LOW)
            GPIO.output(24, GPIO.LOW)
            GPIO.output(25, GPIO.LOW)
            GPIO.output(22, GPIO.LOW)
            time.sleep(0.8)
        time.sleep(0.1)

def stop_motor():
    for pin in motor_pins:
        GPIO.output(pin, GPIO.LOW)

# Start motor thread
#motor_thread = threading.Thread(target=motor_forward)
#motor_thread.start()

# Audio recording parameters
samplerate = 44100
channels = 1
audio_data = []
recording = True

def audio_callback(indata, frames, time_info, status):
    if recording:
        audio_data.append(indata.copy())

def start_audio_stream():
    with sd.InputStream(samplerate=samplerate, channels=channels, callback=audio_callback):
        print("[INFO] Audio stream started...")
        while recording:
            sd.sleep(100)

# Load YOLOv3-tiny
net = cv2.dnn.readNet("yolov3-tiny.weights", "yolov3-tiny.cfg")
net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]
person_index = classes.index("person")
layer_names = net.getUnconnectedOutLayersNames()

cap = cv2.VideoCapture(0)
video_filename = "output_video.avi"
fourcc = cv2.VideoWriter_fourcc(*"XVID")
out = None

audio_thread = threading.Thread(target=start_audio_stream)
audio_thread.start()

print("[INFO] Starting video capture. Press 'q' to stop.")
while True:
    ret, frame = cap.read()
    if not ret:
        break

    if out is None:
        height, width = frame.shape[:2]
        out = cv2.VideoWriter(video_filename, fourcc, 20.0, (width, height))

    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    outputs = net.forward(layer_names)

    boxes, confidences = [], []
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = int(np.argmax(scores))
            confidence = scores[class_id]
            if class_id == person_index and confidence > 0.5:
                center_x, center_y, w, h = (detection[0:4] * [width, height, width, height]).astype("int")
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, int(w), int(h)])
                confidences.append(float(confidence))

    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)
    for i in indices:
        i = int(i)
        x, y, w, h = boxes[i]
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        label = f"Person: {confidences[i]:.2f}"
        cv2.putText(frame, label, (x, y - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    out.write(frame)
    cv2.imshow("YOLOv3-Tiny Person Detection", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Cleanup
print("[INFO] Stopping recording...")
recording = False
motor_running = False
#stop_motor()

cap.release()
if out:
    out.release()
cv2.destroyAllWindows()
audio_thread.join()
motor_thread.join()

# Save audio
if audio_data:
    audio_np = np.concatenate(audio_data, axis=0)
    wav.write("output_audio.wav", samplerate, audio_np)
    print("[INFO] Audio and video saved.")

# Cleanup GPIO
GPIO.cleanup()
